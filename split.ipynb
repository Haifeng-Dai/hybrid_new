{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def simulate_split(K, test_size, num_classes, cncntrtn, mincls, dataset):\n",
    "    \"\"\"Split data indices using labels.\n",
    "\n",
    "    Args:\n",
    "        args (argparser): arguments\n",
    "        dataset (dataset): raw dataset instance to be split\n",
    "\n",
    "    Returns:\n",
    "        split_map (dict): dictionary with key is a client index and a corresponding value is a list of indices\n",
    "\n",
    "    '--K', help='number of total cilents participating in federated training', type=int, default=100\n",
    "    '--test_size', help='a fraction of local hold-out dataset for evaluation (-1 for assigning pre-defined test split as local holdout set)', type=float, choices=[Range(-1, 1.)], default=0.2\n",
    "    '--cncntrtn', help='a concentration parameter for Dirichlet distribution (valid only if `split_type` is `diri`)', type=float, default=0.1\n",
    "    '--mincls', help='the minimum number of distinct classes per client (valid only if `split_type` is `patho` or `diri`)', type=int, default=2\n",
    "    \"\"\"\n",
    "    # Non-IID split proposed in (Hsu et al., 2019); simulation of non-IID split scenario using Dirichlet distribution\n",
    "    MIN_SAMPLES = int(1 / test_size)\n",
    "\n",
    "    total_counts = len(dataset.targets)\n",
    "    _, unique_inverse, unique_counts = np.unique(\n",
    "        dataset.targets, return_inverse=True, return_counts=True)\n",
    "    class_indices = np.split(np.argsort(\n",
    "        unique_inverse), np.cumsum(unique_counts[:-1]))\n",
    "\n",
    "    # calculate ideal samples counts per client\n",
    "    ideal_counts = len(dataset.targets) // K\n",
    "    if ideal_counts < 1:\n",
    "        err = f'[SIMULATE] Decrease the number of participating clients (`args.K` < {K})!'\n",
    "        logger.exception(err)\n",
    "        raise Exception(err)\n",
    "\n",
    "    # split dataset\n",
    "    # define temporary container\n",
    "    assigned_indices = []\n",
    "\n",
    "    # NOTE: it is possible that not all samples be consumed, as it is intended for satisfying each clients having at least `MIN_SAMPLES` samples per class\n",
    "    for k in range(K):\n",
    "        # for current client of which index is `k`\n",
    "        curr_indices = []\n",
    "        satisfied_counts = 0\n",
    "\n",
    "        # ...until the number of samples close to ideal counts is filled\n",
    "        while satisfied_counts < ideal_counts:\n",
    "            # define Dirichlet distribution of which prior distribution is an uniform distribution\n",
    "            diri_prior = np.random.uniform(size=num_classes)\n",
    "\n",
    "            # sample a parameter corresponded to that of categorical distribution\n",
    "            cat_param = np.random.dirichlet(\n",
    "                alpha=cncntrtn * diri_prior)\n",
    "\n",
    "            # try to sample by amount of `ideal_counts``\n",
    "            sampled = np.random.choice(\n",
    "                num_classes, ideal_counts, p=cat_param)\n",
    "\n",
    "            # count per-class samples\n",
    "            unique, counts = np.unique(sampled, return_counts=True)\n",
    "            if len(unique) < mincls:\n",
    "                continue\n",
    "\n",
    "            # filter out sampled classes not having as much as `MIN_SAMPLES`\n",
    "            required_counts = counts * (counts > MIN_SAMPLES)\n",
    "\n",
    "            # assign from population indices split by classes\n",
    "            for idx, required_class in enumerate(unique):\n",
    "                if required_counts[idx] == 0:\n",
    "                    continue\n",
    "                sampled_indices = class_indices[required_class][:required_counts[idx]]\n",
    "                curr_indices.append(sampled_indices)\n",
    "                class_indices[required_class] = class_indices[required_class][:required_counts[idx]]\n",
    "            satisfied_counts += sum(required_counts)\n",
    "\n",
    "        # when enough samples are collected, go to next clients!\n",
    "        assigned_indices.append(np.concatenate(curr_indices))\n",
    "\n",
    "    # construct a hashmap\n",
    "    split_map = {k: assigned_indices[k] for k in range(K)}\n",
    "    return split_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=True,\n",
    "                                     transform=torchvision.transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "K = 10\n",
    "test_size = 0.2\n",
    "num_classes = 10\n",
    "cncntrtn = 0.1\n",
    "mincls = 2\n",
    "split_data = simulate_split(K=K,\n",
    "                            test_size=test_size,\n",
    "                            num_classes=num_classes,\n",
    "                            cncntrtn=cncntrtn,\n",
    "                            mincls=mincls,\n",
    "                            dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "0 (11922,)\n",
      "1 (6000,)\n",
      "2 (5985,)\n",
      "3 (5542,)\n",
      "4 (1931,)\n",
      "5 (108,)\n",
      "6 (130,)\n",
      "7 (1812,)\n",
      "8 (478,)\n",
      "9 (616,)\n"
     ]
    }
   ],
   "source": [
    "print(type(split_data))\n",
    "for i, j in split_data.items():\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.51377238e-02 9.42737249e-01 9.37138777e-03 4.91404895e-04\n",
      " 2.22622343e-02]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.dirichlet(np.repeat(0.1, 5))\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
